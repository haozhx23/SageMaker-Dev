{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c37c63b1-f665-4008-9369-45782fc02d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = sess.boto_session.region_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5125b711-3bf9-4938-8c6f-b4898412ef84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpioptions = \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR \"\n",
    "mpioptions += \"-x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 \"\n",
    "mpioptions += \"-x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63ae28a6-4fa5-4a30-a103-05b7968e1859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.0.0-gpu-py310-cu118-ubuntu20.04-sagemaker'\n",
    "# image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.13.1-gpu-py39-cu117-ubuntu20.04-sagemaker'\n",
    "# image_uri = '763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04'\n",
    "\n",
    "cfg = OmegaConf.load('sdp_llama_7b_zh_instruct_coig.yaml')\n",
    "\n",
    "est = PyTorch(role=role,\n",
    "            entry_point='sdp_trainer_base_ds_mul_aws.py',\n",
    "            source_dir='./',\n",
    "            base_job_name='panda-llm-sdp-job',\n",
    "            instance_count=2,\n",
    "            instance_type='ml.p4d.24xlarge',\n",
    "            image_uri=image_uri,\n",
    "            # framework_version='1.13.1',\n",
    "            # py_version='py39',\n",
    "            distribution={\n",
    "                \"mpi\": {\"enabled\": True,\n",
    "                        \"processes_per_host\": 8,\n",
    "                        \"custom_mpi_options\": mpioptions,\n",
    "                       },\n",
    "                \"smdistributed\": {\n",
    "                        \"modelparallel\": {\n",
    "                            \"enabled\": True,\n",
    "                            \"parameters\": dict(cfg.smp_init_params),\n",
    "                        }\n",
    "                    }\n",
    "            },\n",
    "            max_run=3600*24*2, # 训练任务存续的时间上限\n",
    "            keep_alive_period_in_seconds=3600,\n",
    "            disable_profiler=True,\n",
    "            debugger_hook_config=False\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a8e65-bcdf-42f9-878c-8419c1da7ffe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: panda-llm-sdp-job-2023-05-26-07-48-21-793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-26 07:48:25 Starting - Starting the training job..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:35,398 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:35,457 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:35,464 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:35,466 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:35,371 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:35,430 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:35,438 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:35,439 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:36,873 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 40.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 68.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:36,852 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting wandb (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 55.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting nltk (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 86.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting tensorboard (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 109.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 68.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.28.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 121.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting peft (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 17.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch==2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.0.0)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 92.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 92.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.28.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.0/7.0 MB 111.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting peft (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.3.0-py3-none-any.whl (56 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 18.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch==2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.0.0)\u001b[0m\n",
      "\u001b[34mCollecting hydra-core (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 31.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fairscale (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mDownloading fairscale-0.4.13.tar.gz (266 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.3/266.3 kB 53.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mCollecting hydra-core (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[35mDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 154.5/154.5 kB 39.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting fairscale (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[35mDownloading fairscale-0.4.13.tar.gz (266 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 266.3/266.3 kB 55.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[35mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (3.12.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 48.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (1.23.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (5.4.1)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers==4.28.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.7/769.7 kB 88.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 113.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (4.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (8.1.3)\u001b[0m\n",
      "\u001b[35mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading GitPython-3.1.31-py3-none-any.whl (184 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 44.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[35mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading sentry_sdk-1.24.0-py2.py3-none-any.whl (206 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.5/206.5 kB 42.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting pathtools (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting setproctitle (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[35mCollecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[35mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 29.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.54.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 124.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 41.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.4.3-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 30.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 121.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 3)) (2.3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 3)) (0.40.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 6)) (0.19.0)\u001b[0m\n",
      "\u001b[35mCollecting omegaconf<2.4,>=2.2 (from hydra-core->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[35mDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 19.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting antlr4-python3-runtime==4.9.* (from hydra-core->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[35mDownloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 23.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\n",
      "2023-05-26 07:48:34 Downloading - Downloading input data\n",
      "2023-05-26 07:48:34 Training - Training image download completed. Training in progress.\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (3.12.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.28.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 224.5/224.5 kB 40.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.28.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.5.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (769 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 769.7/769.7 kB 89.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (2.28.2)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.28.1->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 106.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.28.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (1.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.0->-r requirements.txt (line 7)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (8.1.3)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.31-py3-none-any.whl (184 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.3/184.3 kB 38.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.24.0-py2.py3-none-any.whl (206 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.5/206.5 kB 39.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3 (from wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 24.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.54.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 111.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 40.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.4.3-py3-none-any.whl (93 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.9/93.9 kB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 126.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 3)) (2.3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 3)) (0.40.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft->-r requirements.txt (line 6)) (0.19.0)\u001b[0m\n",
      "\u001b[34mCollecting omegaconf<2.4,>=2.2 (from hydra-core->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.5/79.5 kB 18.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting antlr4-python3-runtime==4.9.* (from hydra-core->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 22.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 15.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 43.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (1.26.15)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 3)) (2.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0->-r requirements.txt (line 7)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading gitdb-4.0.10-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 18.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 40.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (4.7.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (1.26.15)\u001b[0m\n",
      "\u001b[35mCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.28.1->-r requirements.txt (line 5)) (2023.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.28.1->-r requirements.txt (line 5)) (2023.5.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 3)) (2.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.0->-r requirements.txt (line 7)) (1.3.0)\u001b[0m\n",
      "\u001b[35mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading smmap-5.0.0-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[35mCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 37.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: antlr4-python3-runtime, fairscale, pathtools\u001b[0m\n",
      "\u001b[35mBuilding wheel for antlr4-python3-runtime (setup.py): started\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 3)) (0.4.8)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 25.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: antlr4-python3-runtime, fairscale, pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=5bf2e6b4b52131c335fb849f428091ec6bd69544176bf1f2e7efd52675821f12\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\u001b[0m\n",
      "\u001b[34mBuilding wheel for fairscale (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fairscale (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332112 sha256=2707b8790ba0c408be1362a01cee8705cc280b15ba432583aca161156d6d86f5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=a6ecc2c301cf1922c879a01b6c1fae056ce720fbabaac5d7d30a61bb2f5b3cd3\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\u001b[0m\n",
      "\u001b[35mBuilding wheel for fairscale (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for fairscale (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for fairscale: filename=fairscale-0.4.13-py3-none-any.whl size=332112 sha256=2707b8790ba0c408be1362a01cee8705cc280b15ba432583aca161156d6d86f5\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/78/a4/c0/fb0a7ef03cff161611c3fa40c6cf898f76e58ec421b88e8cb3\u001b[0m\n",
      "\u001b[35mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ed1797865c52f4f9481f0d72f1d6456850f5bc65474d332a164f7153c9db9dbb\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\u001b[0m\n",
      "\u001b[35mSuccessfully built antlr4-python3-runtime fairscale pathtools\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=ed1797865c52f4f9481f0d72f1d6456850f5bc65474d332a164f7153c9db9dbb\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\u001b[0m\n",
      "\u001b[34mSuccessfully built antlr4-python3-runtime fairscale pathtools\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, sentencepiece, pathtools, appdirs, antlr4-python3-runtime, tensorboard-data-server, smmap, setproctitle, sentry-sdk, regex, pyasn1-modules, omegaconf, oauthlib, markdown, grpcio, docker-pycreds, cachetools, absl-py, requests-oauthlib, nltk, hydra-core, huggingface-hub, google-auth, gitdb, transformers, google-auth-oauthlib, GitPython, fairscale, wandb, tensorboard, peft\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, sentencepiece, pathtools, appdirs, antlr4-python3-runtime, tensorboard-data-server, smmap, setproctitle, sentry-sdk, regex, pyasn1-modules, omegaconf, oauthlib, markdown, grpcio, docker-pycreds, cachetools, absl-py, requests-oauthlib, nltk, hydra-core, huggingface-hub, google-auth, gitdb, transformers, google-auth-oauthlib, GitPython, fairscale, wandb, tensorboard, peft\u001b[0m\n",
      "\u001b[35mSuccessfully installed GitPython-3.1.31 absl-py-1.4.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 cachetools-5.3.0 docker-pycreds-0.4.0 fairscale-0.4.13 gitdb-4.0.10 google-auth-2.19.0 google-auth-oauthlib-1.0.0 grpcio-1.54.2 huggingface-hub-0.14.1 hydra-core-1.3.2 markdown-3.4.3 nltk-3.8.1 oauthlib-3.2.2 omegaconf-2.3.0 pathtools-0.1.2 peft-0.3.0 pyasn1-modules-0.3.0 regex-2023.5.5 requests-oauthlib-1.3.1 sentencepiece-0.1.99 sentry-sdk-1.24.0 setproctitle-1.3.2 smmap-5.0.0 tensorboard-2.13.0 tensorboard-data-server-0.7.0 tokenizers-0.13.3 transformers-4.28.1 wandb-0.15.3\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.31 absl-py-1.4.0 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 cachetools-5.3.0 docker-pycreds-0.4.0 fairscale-0.4.13 gitdb-4.0.10 google-auth-2.19.0 google-auth-oauthlib-1.0.0 grpcio-1.54.2 huggingface-hub-0.14.1 hydra-core-1.3.2 markdown-3.4.3 nltk-3.8.1 oauthlib-3.2.2 omegaconf-2.3.0 pathtools-0.1.2 peft-0.3.0 pyasn1-modules-0.3.0 regex-2023.5.5 requests-oauthlib-1.3.1 sentencepiece-0.1.99 sentry-sdk-1.24.0 setproctitle-1.3.2 smmap-5.0.0 tensorboard-2.13.0 tensorboard-data-server-0.7.0 tokenizers-0.13.3 transformers-4.28.1 wandb-0.15.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,141 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,141 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,203 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,270 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,278 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,278 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,287 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,420 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,420 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,420 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,421 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:55,425 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,005 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,006 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,068 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,135 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,143 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,143 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,144 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,144 sagemaker-training-toolkit INFO     Cannot connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:55,145 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.212.54.              Can be ignored for worker when master completes and exits.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,156 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,285 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,285 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,285 sagemaker-training-toolkit INFO     Worker algo-1 available for communication\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,285 sagemaker-training-toolkit INFO     Env Hosts: ['algo-2', 'algo-1'] Hosts: ['algo-2:8', 'algo-1:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,286 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,347 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,354 sagemaker-training-toolkit INFO     PyTorch version is 2.0.0\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,414 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,422 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train123\": \"/opt/ml/input/data/train123\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-2\",\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"mp_parameters\": {\n",
      "            \"ddp\": true,\n",
      "            \"partitions\": 1,\n",
      "            \"sharded_data_parallel_degree\": 8,\n",
      "            \"offload_activations\": true,\n",
      "            \"fp16\": false,\n",
      "            \"bf16\": true,\n",
      "            \"activation_loading_horizon\": 4,\n",
      "            \"skip_tracing\": true,\n",
      "            \"delayed_parameter_initialization\": true,\n",
      "            \"prescaled_batch\": true\n",
      "        }\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train123\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-2\",\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"panda-llm-sdp-job-2023-05-26-07-48-21-793\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-2\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-633205212955/panda-llm-sdp-job-2023-05-26-07-48-21-793/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sdp_trainer_base_ds_mul_aws\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-2\",\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sdp_trainer_base_ds_mul_aws.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"mp_parameters\":{\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":true,\"partitions\":1,\"prescaled_batch\":true,\"sharded_data_parallel_degree\":8,\"skip_tracing\":true}}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=sdp_trainer_base_ds_mul_aws.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train123\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train123\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-2\",\"algo-1\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=sdp_trainer_base_ds_mul_aws\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-633205212955/panda-llm-sdp-job-2023-05-26-07-48-21-793/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"-x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"train123\":\"/opt/ml/input/data/train123\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-2\",\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-2\",\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"mp_parameters\":{\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":true,\"partitions\":1,\"prescaled_batch\":true,\"sharded_data_parallel_degree\":8,\"skip_tracing\":true}},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train123\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"panda-llm-sdp-job-2023-05-26-07-48-21-793\",\"log_level\":20,\"master_hostname\":\"algo-2\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-633205212955/panda-llm-sdp-job-2023-05-26-07-48-21-793/source/sourcedir.tar.gz\",\"module_name\":\"sdp_trainer_base_ds_mul_aws\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-2\",\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sdp_trainer_base_ds_mul_aws.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--mp_parameters\",\"activation_loading_horizon=4,bf16=True,ddp=True,delayed_parameter_initialization=True,fp16=False,offload_activations=True,partitions=1,prescaled_batch=True,sharded_data_parallel_degree=8,skip_tracing=True\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN123=/opt/ml/input/data/train123\u001b[0m\n",
      "\u001b[35mSM_HP_MP_PARAMETERS={\"activation_loading_horizon\":4,\"bf16\":true,\"ddp\":true,\"delayed_parameter_initialization\":true,\"fp16\":false,\"offload_activations\":true,\"partitions\":1,\"prescaled_batch\":true,\"sharded_data_parallel_degree\":8,\"skip_tracing\":true}\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mmpirun --host algo-2:8,algo-1:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.10/site-packages/gethostname.cpython-310-x86_64-linux-gnu.so -x NCCL_DEBUG=WARN -x SMDEBUG_LOG_LEVEL=ERROR -x SMP_DISABLE_D2D=1 -x SMP_D2D_GPU_BUFFER_SIZE_BYTES=1 -x SMP_NCCL_THROTTLE_LIMIT=1 -x FI_EFA_USE_DEVICE_RDMA=1 -x FI_PROVIDER=efa -x RDMAV_FORK_SAFE=1 -x FI_PROVIDER=efa -x NCCL_PROTO=simple -x FI_EFA_USE_DEVICE_RDMA=1 -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_NUM_NEURONS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TRAIN123 -x SM_HP_MP_PARAMETERS -x PYTHONPATH -x NCCL_PROTO=simple -x NCCL_ALGO=ring smddpmprun -i ml.p4d.24xlarge --allow-bypass /opt/conda/bin/python3.10 -m mpi4py sdp_trainer_base_ds_mul_aws.py --mp_parameters activation_loading_horizon=4,bf16=True,ddp=True,delayed_parameter_initialization=True,fp16=False,offload_activations=True,partitions=1,prescaled_batch=True,sharded_data_parallel_degree=8,skip_tracing=True\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:56,481 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:58,433 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=107, name='orted', status='sleeping', started='07:48:57')]\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:58,433 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=107, name='orted', status='sleeping', started='07:48:57')]\u001b[0m\n",
      "\u001b[34m2023-05-26 07:48:58,433 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=107, name='orted', status='sleeping', started='07:48:57')]\u001b[0m\n",
      "\u001b[35m[2023-05-26 07:48:57.907: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m2023-05-26 07:48:57,920 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[35mWarning: Permanently added 'algo-1,10.0.212.54' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[35mData for JOB [22858,1] offset 0 Total slots allocated 16\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [22858,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:49:01.303: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:49:01.315: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:49:01.324: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:49:01.324: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:49:01.368: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:49:01.368: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:49:01.385: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:49:01.385: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:49:01.395: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:49:01.396: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:01.424: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:49:01.424: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:49:01.424: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:49:01.459: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:49:01.508: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:49:01.551: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.28.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:49:08.506: I smdistributed/modelparallel/torch/state_mod.py:100] [10] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:49:08.522: I smdistributed/modelparallel/torch/state_mod.py:100] [11] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:49:08.523: I smdistributed/modelparallel/torch/state_mod.py:100] [15] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:49:08.528: I smdistributed/modelparallel/torch/state_mod.py:100] [9] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:49:08.530: I smdistributed/modelparallel/torch/state_mod.py:100] [12] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:49:08.531: I smdistributed/modelparallel/torch/state_mod.py:100] [8] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:08.536: I smdistributed/modelparallel/torch/state_mod.py:100] [0] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:49:08.543: I smdistributed/modelparallel/torch/state_mod.py:100] [13] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:49:08.544: I smdistributed/modelparallel/torch/state_mod.py:100] [14] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:49:08.552: I smdistributed/modelparallel/torch/state_mod.py:100] [2] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:49:08.553: I smdistributed/modelparallel/torch/state_mod.py:100] [1] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:49:08.557: I smdistributed/modelparallel/torch/state_mod.py:100] [5] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:49:08.561: I smdistributed/modelparallel/torch/state_mod.py:100] [3] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:49:08.565: I smdistributed/modelparallel/torch/state_mod.py:100] [6] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:49:08.571: I smdistributed/modelparallel/torch/state_mod.py:100] [4] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:49:08.576: I smdistributed/modelparallel/torch/state_mod.py:100] [7] Initializing torch distributed process groups with smddp backend\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:Successfully opened device efa_3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:Successfully opened device efa_1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:Successfully opened device efa_2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:Successfully opened device efa_2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:Successfully opened device efa_1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:Successfully opened device efa_0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:Successfully opened device efa_3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:Successfully opened device efa_0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:Successfully opened device efa_0\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:Successfully opened device efa_2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Successfully opened device efa_0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:Successfully opened device efa_1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:Successfully opened device efa_1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:Successfully opened device efa_3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:Successfully opened device efa_2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:Successfully opened device efa_3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= [1,mpirank:15,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:All-Gather Buffer Manager config sort_buffer_size= 134217728 scratch_buffer_size= 16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:16777216\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Running smdistributed.dataparallel v1.8.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:NCCL version 2.16.2+cuda11.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [5] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 5, rdp_rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [3] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 3, rdp_rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [4] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 4, rdp_rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [7] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 7, rdp_rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [12] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 12, rdp_rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [13] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 13, rdp_rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [11] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 11, rdp_rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [15] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 15, rdp_rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [10] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 10, rdp_rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [8] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 8, rdp_rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [14] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 14, rdp_rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:49:15.169: I smdistributed/modelparallel/torch/state_mod.py:163] [9] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 9, rdp_rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.179: I smdistributed/modelparallel/torch/state_mod.py:163] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:49:15.179: I smdistributed/modelparallel/torch/state_mod.py:163] [1] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 1, rdp_rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:49:15.179: I smdistributed/modelparallel/torch/state_mod.py:163] [2] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 2, rdp_rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:49:15.179: I smdistributed/modelparallel/torch/state_mod.py:163] [6] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 6, rdp_rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.179: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 1.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.179: I smdistributed/modelparallel/backend/config.py:314] Configuration parameters:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   active_microbatches: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   auto_partition: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   bf16: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   contiguous: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   ddp: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   ddp_dist_backend: auto\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   ddp_port: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   default_partition: None\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   delayed_parameter_initialization: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   fp16: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   fp16_params: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   horovod: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   microbatches: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   offload_activations: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   optimize: speed\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   pipeline: interleaved\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.180: I smdistributed/modelparallel/backend/config.py:317]   pipeline_parallel_degree: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   predefined_hooks: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   prescaled_batch: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   shard_optimizer_state: False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   sharded_data_parallel_degree: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   skip_tracing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: I smdistributed/modelparallel/backend/config.py:317]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.181: W smdistributed/modelparallel/backend/config.py:323] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:49:15.600: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:49:15.601: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:49:15.601: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:49:15.601: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:49:15.601: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:49:15.602: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:49:15.602: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:49:15.602: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:49:15.603: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:49:15.603: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:49:15.604: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:49:15.604: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:49:15.606: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:49:15.608: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:49:15.646: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:49:15.646: W smdistributed/modelparallel/torch/__init__.py:115] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/generation_config.json /tmp/llama_pretrain/generation_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/special_tokens_map.json /tmp/llama_pretrain/special_tokens_map.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model.bin.index.json /tmp/llama_pretrain/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/tokenizer_config.json /tmp/llama_pretrain/tokenizer_config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/config.json /tmp/llama_pretrain/config.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/tokenizer.model /tmp/llama_pretrain/tokenizer.model\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00019-of-00033.bin /tmp/llama_pretrain/pytorch_model-00019-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00019-of-00033.bin /tmp/llama_pretrain/pytorch_model-00019-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00004-of-00033.bin /tmp/llama_pretrain/pytorch_model-00004-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00004-of-00033.bin /tmp/llama_pretrain/pytorch_model-00004-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00028-of-00033.bin /tmp/llama_pretrain/pytorch_model-00028-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00031-of-00033.bin /tmp/llama_pretrain/pytorch_model-00031-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00024-of-00033.bin /tmp/llama_pretrain/pytorch_model-00024-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00028-of-00033.bin /tmp/llama_pretrain/pytorch_model-00028-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00017-of-00033.bin /tmp/llama_pretrain/pytorch_model-00017-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00003-of-00033.bin /tmp/llama_pretrain/pytorch_model-00003-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00031-of-00033.bin /tmp/llama_pretrain/pytorch_model-00031-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00024-of-00033.bin /tmp/llama_pretrain/pytorch_model-00024-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00007-of-00033.bin /tmp/llama_pretrain/pytorch_model-00007-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00016-of-00033.bin /tmp/llama_pretrain/pytorch_model-00016-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00025-of-00033.bin /tmp/llama_pretrain/pytorch_model-00025-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00017-of-00033.bin /tmp/llama_pretrain/pytorch_model-00017-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00016-of-00033.bin /tmp/llama_pretrain/pytorch_model-00016-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00013-of-00033.bin /tmp/llama_pretrain/pytorch_model-00013-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00013-of-00033.bin /tmp/llama_pretrain/pytorch_model-00013-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00015-of-00033.bin /tmp/llama_pretrain/pytorch_model-00015-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00006-of-00033.bin /tmp/llama_pretrain/pytorch_model-00006-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00007-of-00033.bin /tmp/llama_pretrain/pytorch_model-00007-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00015-of-00033.bin /tmp/llama_pretrain/pytorch_model-00015-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00006-of-00033.bin /tmp/llama_pretrain/pytorch_model-00006-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00002-of-00033.bin /tmp/llama_pretrain/pytorch_model-00002-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00002-of-00033.bin /tmp/llama_pretrain/pytorch_model-00002-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00003-of-00033.bin /tmp/llama_pretrain/pytorch_model-00003-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00010-of-00033.bin /tmp/llama_pretrain/pytorch_model-00010-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00010-of-00033.bin /tmp/llama_pretrain/pytorch_model-00010-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00025-of-00033.bin /tmp/llama_pretrain/pytorch_model-00025-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00022-of-00033.bin /tmp/llama_pretrain/pytorch_model-00022-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00029-of-00033.bin /tmp/llama_pretrain/pytorch_model-00029-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00029-of-00033.bin /tmp/llama_pretrain/pytorch_model-00029-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00022-of-00033.bin /tmp/llama_pretrain/pytorch_model-00022-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00011-of-00033.bin /tmp/llama_pretrain/pytorch_model-00011-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00011-of-00033.bin /tmp/llama_pretrain/pytorch_model-00011-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00032-of-00033.bin /tmp/llama_pretrain/pytorch_model-00032-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00032-of-00033.bin /tmp/llama_pretrain/pytorch_model-00032-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00018-of-00033.bin /tmp/llama_pretrain/pytorch_model-00018-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00020-of-00033.bin /tmp/llama_pretrain/pytorch_model-00020-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00020-of-00033.bin /tmp/llama_pretrain/pytorch_model-00020-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00018-of-00033.bin /tmp/llama_pretrain/pytorch_model-00018-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00027-of-00033.bin /tmp/llama_pretrain/pytorch_model-00027-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00008-of-00033.bin /tmp/llama_pretrain/pytorch_model-00008-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00027-of-00033.bin /tmp/llama_pretrain/pytorch_model-00027-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00008-of-00033.bin /tmp/llama_pretrain/pytorch_model-00008-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00012-of-00033.bin /tmp/llama_pretrain/pytorch_model-00012-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00014-of-00033.bin /tmp/llama_pretrain/pytorch_model-00014-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00012-of-00033.bin /tmp/llama_pretrain/pytorch_model-00012-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00021-of-00033.bin /tmp/llama_pretrain/pytorch_model-00021-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00014-of-00033.bin /tmp/llama_pretrain/pytorch_model-00014-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00030-of-00033.bin /tmp/llama_pretrain/pytorch_model-00030-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00023-of-00033.bin /tmp/llama_pretrain/pytorch_model-00023-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00030-of-00033.bin /tmp/llama_pretrain/pytorch_model-00030-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00023-of-00033.bin /tmp/llama_pretrain/pytorch_model-00023-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00005-of-00033.bin /tmp/llama_pretrain/pytorch_model-00005-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00021-of-00033.bin /tmp/llama_pretrain/pytorch_model-00021-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00005-of-00033.bin /tmp/llama_pretrain/pytorch_model-00005-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00001-of-00033.bin /tmp/llama_pretrain/pytorch_model-00001-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00001-of-00033.bin /tmp/llama_pretrain/pytorch_model-00001-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00033-of-00033.bin /tmp/llama_pretrain/pytorch_model-00033-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00033-of-00033.bin /tmp/llama_pretrain/pytorch_model-00033-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00026-of-00033.bin /tmp/llama_pretrain/pytorch_model-00026-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00026-of-00033.bin /tmp/llama_pretrain/pytorch_model-00026-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00009-of-00033.bin /tmp/llama_pretrain/pytorch_model-00009-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:cp s3://llm-artifacts-us-east-1/decapoda-research-llama-7b-hf/pytorch_model-00009-of-00033.bin /tmp/llama_pretrain/pytorch_model-00009-of-00033.bin\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizerFast(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- smp.model_creation\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:15,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.89it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.90it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.88it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:17,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:14,  1.88it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:18,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:15,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:17,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:13,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:15,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:17,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:14,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:17,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:15,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:13,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:17,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.80it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:16,  1.92it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:17,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:12,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:18,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:14,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:13,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:17,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:16,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:14,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:16,  1.91it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:17,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:12,  1.80it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:18,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.80it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:15,  1.91it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:16,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:11,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.models.llama:gradient_checkpointing: True\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:16,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:18,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:17,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:13,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:12,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:15,  1.91it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:15,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:13,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:17,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:17,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:13,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:14,  1.90it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:13,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:15,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:18,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:17,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:15,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:12,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:11,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:16,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:14,  1.88it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:12,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   3%|▎         | 1/33 [00:00<00:18,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:18,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:13,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   6%|▌         | 2/33 [00:01<00:18,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:03<00:14,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:09,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:17,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:16,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:11,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:14,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:15,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:13,  1.87it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:11,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   9%|▉         | 3/33 [00:01<00:17,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:08,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:09,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:16,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.80it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:04<00:12,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  12%|█▏        | 4/33 [00:02<00:17,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:08,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:09,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:16,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:15,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:10,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:12,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:14,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:11,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  15%|█▌        | 5/33 [00:02<00:16,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:08,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:05<00:11,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:09,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  18%|█▊        | 6/33 [00:03<00:15,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:08,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:11,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:15,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:09,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  21%|██        | 7/33 [00:04<00:15,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:06<00:10,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:08,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:11,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:07<00:12,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:07,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  24%|██▍       | 8/33 [00:04<00:14,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.83it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:13,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:08,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:10,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:05,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:07<00:10,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  27%|██▋       | 9/33 [00:05<00:14,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:09,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.81it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:10,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:11,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  30%|███       | 10/33 [00:05<00:13,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:08<00:09,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:05,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:04,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:11<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:08,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  33%|███▎      | 11/33 [00:06<00:12,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:06<00:12,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:10,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:09<00:08,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  36%|███▋      | 12/33 [00:07<00:12,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:04,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:11<00:08,  1.68it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:14<00:03,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:07,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:06,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  39%|███▉      | 13/33 [00:07<00:11,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:05,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:05,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:11,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.68it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:10<00:07,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:10<00:08,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:04,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  42%|████▏     | 14/33 [00:08<00:11,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:14<00:03,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:11<00:08,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:07,  1.68it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:11<00:06,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:04,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  45%|████▌     | 15/33 [00:08<00:10,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:04,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:05,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:11<00:05,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:07,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:16<00:01,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.68it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:06,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  48%|████▊     | 16/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:15<00:02,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:11<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:12<00:05,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:07,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:14<00:03,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  52%|█████▏    | 17/33 [00:09<00:09,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:16<00:01,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:12<00:04,  1.80it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:17<00:00,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:14<00:05,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:15<00:03,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  55%|█████▍    | 18/33 [00:10<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:16<00:01,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:11<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:13<00:04,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:15<00:02,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.67it/s][1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:04,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  58%|█████▊    | 19/33 [00:11<00:08,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:17<00:00,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:03,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:16<00:01,  1.85it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:14<00:05,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:15<00:04,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:16<00:01,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  61%|██████    | 20/33 [00:11<00:07,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.74it/s][1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:17<00:00,  1.84it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:16<00:01,  1.86it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:14<00:03,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:15<00:04,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:14<00:04,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  64%|██████▎   | 21/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:17<00:00,  1.88it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:15<00:02,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:15<00:04,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:17<00:00,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  67%|██████▋   | 22/33 [00:12<00:06,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:16<00:01,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:15<00:02,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:17<00:02,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.69it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  70%|██████▉   | 23/33 [00:13<00:05,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.79it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:15<00:04,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:13<00:05,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:16<00:01,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.80it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  73%|███████▎  | 24/33 [00:14<00:05,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.80it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:16<00:01,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:18<00:01,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  76%|███████▌  | 25/33 [00:14<00:04,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:17<00:00,  1.78it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:15<00:04,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:18<00:01,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  79%|███████▉  | 26/33 [00:15<00:04,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.62it/s][1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.68it/s][1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:18<00:00,  1.82it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:18<00:01,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.63it/s][1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.70it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  82%|████████▏ | 27/33 [00:15<00:03,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.63it/s][1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  85%|████████▍ | 28/33 [00:16<00:02,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.65it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:17<00:01,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.73it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.66it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  88%|████████▊ | 29/33 [00:16<00:02,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.74it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  91%|█████████ | 30/33 [00:17<00:01,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:18<00:01,  1.75it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.67it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  94%|█████████▍| 31/33 [00:18<00:01,  1.76it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  97%|█████████▋| 32/33 [00:18<00:00,  1.77it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.69it/s][1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.72it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.69it/s][1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|██████████| 33/33 [00:19<00:00,  1.71it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.models.llama:Config pad token id after loading pre-trained weights: 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:53:44.326: I smdistributed/modelparallel/torch/model.py:153] [2] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:53:44,329] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:53:44.574: I smdistributed/modelparallel/torch/model.py:153] [6] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:53:44,577] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:53:45.928: I smdistributed/modelparallel/torch/model.py:153] [5] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:53:45,931] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:53:46.866: I smdistributed/modelparallel/torch/model.py:153] [4] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:53:46,869] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:53:47.042: I smdistributed/modelparallel/torch/model.py:153] [1] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:53:47,045] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:53:47.757: I smdistributed/modelparallel/torch/model.py:153] [0] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:53:47,760] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:shard size 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:mp size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:53:48.195: I smdistributed/modelparallel/torch/model.py:153] [3] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:53:48,197] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:53:48.728: I smdistributed/modelparallel/torch/model.py:153] [10] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:53:48,731] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:53:48.930: I smdistributed/modelparallel/torch/model.py:153] [9] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:53:48,933] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:53:49.326: I smdistributed/modelparallel/torch/model.py:153] [15] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:53:49,328] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:53:49.836: I smdistributed/modelparallel/torch/model.py:153] [12] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:53:49,838] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:53:49.935: I smdistributed/modelparallel/torch/model.py:153] [8] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:53:49,937] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:53:50.072: I smdistributed/modelparallel/torch/model.py:153] [7] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:53:50,074] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:53:51.159: I smdistributed/modelparallel/torch/model.py:153] [14] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:53:51,161] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:53:52.351: I smdistributed/modelparallel/torch/model.py:153] [13] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:53:52,353] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:53:52.660: I smdistributed/modelparallel/torch/model.py:153] [11] Bit16_Module initialized, using dtype torch.bfloat16\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:53:52,663] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:52 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5, model_parallel_rank 0, shard group 5/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2, model_parallel_rank 0, shard group 2/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4, model_parallel_rank 0, shard group 4/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7, model_parallel_rank 0, shard group 7/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3, model_parallel_rank 0, shard group 3/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1, model_parallel_rank 0, shard group 1/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0, model_parallel_rank 0, shard group 0/8[1,mpirank:0,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6, model_parallel_rank 0, shard group 6/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:52 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9, model_parallel_rank 0, shard group 1/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10, model_parallel_rank 0, shard group 2/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12, model_parallel_rank 0, shard group 4/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11, model_parallel_rank 0, shard group 3/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:53 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15, model_parallel_rank 0, shard group 7/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14, model_parallel_rank 0, shard group 6/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13, model_parallel_rank 0, shard group 5/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8, model_parallel_rank 0, shard group 0/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:53 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:54 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:54 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:55 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:55 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:56 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:56 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:57 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:57 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:58 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:58 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:59 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:59 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:60 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:60 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:61 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:created shard groups and replicate groups based on shard size 8 and mp size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:nn.functional.linear has been overridden with a more memory efficient version. This will persist unless manually reset.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:61 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:53:58,825] [INFO] [partition_parameters.py:519:__exit__] finished initializing model with 0.00B parameters\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:53:58,839] [INFO] [stage3.py:661:__init__] Reduce bucket size 500000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:53:58,839] [INFO] [stage3.py:662:__init__] Allgather bucket size 50000000\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Creating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Building extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Creating extension directory /root/.cache/torch_extensions/py310_cu118/utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Emitting ninja build file /root/.cache/torch_extensions/py310_cu118/utils/build.ninja...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Building extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Using /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[1/2] c++ -MMD -MF flatten_unflatten.o.d -DTORCH_EXTENSION_NAME=utils -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/utils/flatten_unflatten.cpp -o flatten_unflatten.o\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:Time to load utils op: 13.536501407623291 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:54:13,104] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:Time to load utils op: 13.519312381744385 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:Time to load utils op: 13.4221773147583 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:Time to load utils op: 13.423465013504028 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:54:13,135] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:Time to load utils op: 13.422854661941528 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:Time to load utils op: 13.420760869979858 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:54:13,138] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:54:13,138] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:54:13,139] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:54:13,139] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:Time to load utils op: 13.42161774635315 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:54:13,147] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2/2] c++ flatten_unflatten.o -shared -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -ltorch_cpu -ltorch -ltorch_python -o utils.so\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:Time to load utils op: 13.565687894821167 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:54:13,190] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:Time to load utils op: 13.417598962783813 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:54:13,195] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:Time to load utils op: 13.519916534423828 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:54:13,209] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:Time to load utils op: 13.522430419921875 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:54:13,234] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:Time to load utils op: 13.520639181137085 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:54:13,255] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:Time to load utils op: 13.522249221801758 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:54:13,274] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:Time to load utils op: 13.520336389541626 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:54:13,279] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:Time to load utils op: 13.619707345962524 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Loading extension module utils...\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:13,283] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:shard size 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:mp size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:Time to load utils op: 13.521683931350708 seconds\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:54:13,287] [INFO] [partition_parameters.py:824:_init_zero2d_config] partition parameters context: local shard True, shard_size \"8\", hierarchy all-gather False\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6 enable local shard at partition parameters Init\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4, model_parallel_rank 0, shard group 4/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2, model_parallel_rank 0, shard group 2/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3, model_parallel_rank 0, shard group 3/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7, model_parallel_rank 0, shard group 7/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6, model_parallel_rank 0, shard group 6/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0, model_parallel_rank 0, shard group 0/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1, model_parallel_rank 0, shard group 1/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5, model_parallel_rank 0, shard group 5/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:62 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:62 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:63 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10, model_parallel_rank 0, shard group 2/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9, model_parallel_rank 0, shard group 1/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15, model_parallel_rank 0, shard group 7/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14, model_parallel_rank 0, shard group 6/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12, model_parallel_rank 0, shard group 4/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11, model_parallel_rank 0, shard group 3/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8, model_parallel_rank 0, shard group 0/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13, model_parallel_rank 0, shard group 5/8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:63 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:rank 0, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:64 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:rank 8, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:64 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:65 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:rank 1, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:rank 9, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:65 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:rank 2, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:66 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:rank 10, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:66 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:rank 3, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:67 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:rank 11, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:67 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:68 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:rank 4, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:rank 12, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:68 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:69 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:rank 13, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:69 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:rank 5, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:70 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:rank 6, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:rank 14, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:70 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:71 to store for rank: 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:rank 7, model_parallel_rank 0, replicate group 0/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:rank 15, model_parallel_rank 0, replicate group 1/2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:created shard groups and replicate groups based on shard size 8 and mp size 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:71 with 16 nodes.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:54:14,649] [INFO] [stage3.py:1036:_zero2d_setups] rank 6, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:54:14,650] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 6 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:[2023-05-26 07:54:14,651] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f134c17e1b0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f134c17d530> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:54:14,659] [INFO] [stage3.py:1036:_zero2d_setups] rank 5, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:54:14,660] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 5 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:[2023-05-26 07:54:14,660] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f5e92e69630>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f5e86f53c30> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:54:14,858] [INFO] [stage3.py:1036:_zero2d_setups] rank 2, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:54:14,859] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 2 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:[2023-05-26 07:54:14,859] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f043424d630>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f043424ca70> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:54:14,942] [INFO] [stage3.py:1036:_zero2d_setups] rank 13, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:54:14,943] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 13 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:[2023-05-26 07:54:14,943] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f04be780bf0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f04be781530> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:54:14,946] [INFO] [stage3.py:1036:_zero2d_setups] rank 1, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:54:14,947] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 1 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:[2023-05-26 07:54:14,947] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f47893b5270>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f47893b45f0> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:54:14,961] [INFO] [stage3.py:1036:_zero2d_setups] rank 4, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:54:14,962] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 4 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:[2023-05-26 07:54:14,962] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fdf77d3ce30>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7fdf77d3e530> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:54:15,003] [INFO] [stage3.py:1036:_zero2d_setups] rank 3, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:54:15,004] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 3 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:[2023-05-26 07:54:15,004] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7ff6bc9df2f0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7ff6bcb6cab0> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:54:15,008] [INFO] [stage3.py:1036:_zero2d_setups] rank 7, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:54:15,009] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 7 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:[2023-05-26 07:54:15,010] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7efca62ed5b0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7efca62eec30> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:54:15,010] [INFO] [stage3.py:1036:_zero2d_setups] rank 11, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:54:15,011] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 11 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:[2023-05-26 07:54:15,012] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f94e42e1970>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f94e42e02f0> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:15,034] [INFO] [stage3.py:1036:_zero2d_setups] rank 0, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:15,035] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 0 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:15,036] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f646e448670>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f646e44bfb0> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:54:15,040] [INFO] [stage3.py:1036:_zero2d_setups] rank 15, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:54:15,041] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 15 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:[2023-05-26 07:54:15,041] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f759131a470>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f759131b230> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:54:15,155] [INFO] [stage3.py:1036:_zero2d_setups] rank 9, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:54:15,156] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 9 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:[2023-05-26 07:54:15,156] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f523c8a20f0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f523c8a05b0> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:54:15,156] [INFO] [stage3.py:1036:_zero2d_setups] rank 14, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:54:15,157] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 14 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:[2023-05-26 07:54:15,157] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f123d7d40f0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f123d7d4d30> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:54:15,173] [INFO] [stage3.py:1036:_zero2d_setups] rank 10, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:54:15,174] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 10 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:[2023-05-26 07:54:15,174] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f85845216b0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f8584520670> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:54:15,175] [INFO] [stage3.py:1036:_zero2d_setups] rank 12, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:54:15,176] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 12 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:[2023-05-26 07:54:15,176] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f74cee42130>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f74a070a330> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:54:15,193] [INFO] [stage3.py:1036:_zero2d_setups] rank 8, local shard True\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:54:15,194] [INFO] [stage3.py:1045:_zero2d_config_shard_groups] rank 8 enable local shard at DS stage3 optimizer\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:[2023-05-26 07:54:15,194] [INFO] [stage3.py:1051:_zero2d_config_shard_groups] ds_param_shard_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f3540f80bb0>, ds_param_repli_group <torch.distributed.ProcessGroupHerringLMC object at 0x7f3540f83f70> ds_param_shard_size 8 ds_param_repli_size 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:17,400] [INFO] [stage3.py:875:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:17,401] [INFO] [stage3.py:913:__init__] optimizer state initialized\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  1\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  4\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- after OmegaConf.save() to training_config.yaml\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  0\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  0\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- in _load_and_cache_examples, local rank =  6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- before main.train() ..\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- in _load_and_cache_examples, local rank =  2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loading data from /opt/ml/input/data/train123/human_value_alignment_instructions_part1_new.json\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:   0%|          | 0/23 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:   0%|          | 0/23 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.ZN_INSTRUCT:Loaded 2998 examples.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:17.856: I smdistributed/modelparallel/torch/worker.py:300] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:LlamaTokenizer(name_or_path='/tmp/llama_pretrain', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False)\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:INFO:FK.general_util.tokenization_utils:PAD TOKEN ID = 32000\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:17.966: I smdistributed/modelparallel/torch/model.py:665] Partition assignments:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:17.966: I smdistributed/modelparallel/torch/model.py:674] main: 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:17.975: I smdistributed/modelparallel/torch/model.py:599] Number of parameters on partition 0 are 291. 291 require grads\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:17.977: I smdistributed/modelparallel/torch/model.py:628] Number of buffers on partition 0 are 96.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:18.071: I smdistributed/modelparallel/torch/model.py:725] Finished partitioning the model\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:542: UserWarning: torch.distributed.distributed_c10d._get_global_rank is deprecated please use torch.distributed.distributed_c10d.get_global_rank instead\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:[2023-05-26 07:54:18.891: I smdistributed/modelparallel/torch/model.py:734] Broadcasted parameters and buffers for partition 0\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:2547: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:3015: UserWarning: torch.distributed._reduce_scatter_base is a private function and will be deprecated. Please use torch.distributed.reduce_scatter_tensor instead.\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7265625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4296875\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7109375\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.421875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:   4%|▍         | 1/23 [00:03<01:17,  3.53s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 1\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:   4%|▍         | 1/23 [00:03<01:17,  3.54s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4375\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6328125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6328125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:   9%|▊         | 2/23 [00:05<00:52,  2.50s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:   9%|▊         | 2/23 [00:05<00:52,  2.50s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 2\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.7421875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4375\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.359375\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.828125\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  13%|█▎        | 3/23 [00:07<00:43,  2.17s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  13%|█▎        | 3/23 [00:07<00:43,  2.17s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 3\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6875\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.3828125\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7421875\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7890625\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  17%|█▋        | 4/23 [00:08<00:38,  2.01s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  17%|█▋        | 4/23 [00:08<00:38,  2.01s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 4\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.7578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4140625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  22%|██▏       | 5/23 [00:10<00:34,  1.92s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  22%|██▏       | 5/23 [00:10<00:34,  1.92s/it][1,mpirank:8,algo-2]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 5\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.3984375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.46875\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  26%|██▌       | 6/23 [00:12<00:31,  1.86s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 6[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 6\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  26%|██▌       | 6/23 [00:12<00:31,  1.86s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6328125\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.7734375\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.734375\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  30%|███       | 7/23 [00:14<00:29,  1.82s/it][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  30%|███       | 7/23 [00:14<00:29,  1.82s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 7\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  35%|███▍      | 8/23 [00:15<00:27,  1.81s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  35%|███▍      | 8/23 [00:15<00:27,  1.81s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 8\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4140625\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.359375\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  39%|███▉      | 9/23 [00:17<00:24,  1.79s/it]#033[A[1,mpirank:0,algo-1]<stderr>:#015Iteration:  39%|███▉      | 9/23 [00:17<00:24,  1.79s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 9\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.3828125\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.296875\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7265625\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  43%|████▎     | 10/23 [00:19<00:23,  1.77s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  43%|████▎     | 10/23 [00:19<00:23,  1.77s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 10\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.90625\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.46875\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7109375\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.734375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4375\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  48%|████▊     | 11/23 [00:21<00:21,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  48%|████▊     | 11/23 [00:21<00:21,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 11\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6328125\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4375\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4140625\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  52%|█████▏    | 12/23 [00:22<00:19,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  52%|█████▏    | 12/23 [00:22<00:19,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 12\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.8046875\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4296875\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.3828125\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  57%|█████▋    | 13/23 [00:24<00:17,  1.76s/it]#033[A[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  57%|█████▋    | 13/23 [00:24<00:17,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 13\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7421875\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.46875\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  61%|██████    | 14/23 [00:26<00:15,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  61%|██████    | 14/23 [00:26<00:15,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 14\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.3984375\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  65%|██████▌   | 15/23 [00:28<00:14,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  65%|██████▌   | 15/23 [00:28<00:14,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 15\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.8125\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6328125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.78125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5390625\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  70%|██████▉   | 16/23 [00:29<00:12,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 16[1,mpirank:12,algo-2]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  70%|██████▉   | 16/23 [00:29<00:12,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 16\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6328125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4296875\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4296875\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.3984375\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 17[1,mpirank:4,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  74%|███████▍  | 17/23 [00:31<00:10,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  74%|███████▍  | 17/23 [00:31<00:10,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 17\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6328125\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.34375\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.7734375\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.71875\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  78%|███████▊  | 18/23 [00:33<00:08,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  78%|███████▊  | 18/23 [00:33<00:08,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 18\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.734375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.515625\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.7734375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.46875\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.3671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  83%|████████▎ | 19/23 [00:35<00:06,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  83%|████████▎ | 19/23 [00:35<00:06,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 19\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.375\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.46875\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.7109375\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 20[1,mpirank:1,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  87%|████████▋ | 20/23 [00:36<00:05,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 20\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  87%|████████▋ | 20/23 [00:36<00:05,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6875\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.59375\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.53125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5078125\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 21[1,mpirank:2,algo-1]<stdout>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  91%|█████████▏| 21/23 [00:38<00:03,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  91%|█████████▏| 21/23 [00:38<00:03,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 21\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.78125\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.65625\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.7578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.484375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6796875\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.5859375\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4609375\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4765625\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration:  96%|█████████▌| 22/23 [00:40<00:01,  1.75s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration:  96%|█████████▌| 22/23 [00:40<00:01,  1.75s/it][1,mpirank:0,algo-1]<stderr>:#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 22\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6953125\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- In @smp.step loss is 1.625\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4140625\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- In @smp.step loss is 1.6171875\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- In @smp.step loss is 1.703125\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- In @smp.step loss is 1.6015625\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4453125\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- In @smp.step loss is 1.546875\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5234375\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- In @smp.step loss is 1.5625\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- In @smp.step loss is 1.359375\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- In @smp.step loss is 1.640625\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- In @smp.step loss is 1.578125\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- In @smp.step loss is 1.4921875\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration: 100%|██████████| 23/23 [00:42<00:00,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration: 100%|██████████| 23/23 [00:42<00:00,  1.74s/it]#033[A\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Iteration: 100%|██████████| 23/23 [00:42<00:00,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Iteration: 100%|██████████| 23/23 [00:42<00:00,  1.83s/it]\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stderr>:#015Epoch:   0%|          | 0/2 [00:42<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stderr>:#015Epoch:   0%|          | 0/2 [00:42<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:-DD- global_step 23\u001b[0m\n",
      "\u001b[35m[1,mpirank:5,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.5143229961395264\u001b[0m\n",
      "\u001b[35m[1,mpirank:4,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.5442708730697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:1,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.5022786855697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:6,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.5152995586395264\u001b[0m\n",
      "\u001b[35m[1,mpirank:2,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.5263671875\u001b[0m\n",
      "\u001b[35m[1,mpirank:8,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.5231120586395264\u001b[0m\n",
      "\u001b[35m[1,mpirank:15,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.4993489980697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:10,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.5003255605697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:3,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.5100911855697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:7,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.5348308086395264\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- global_step = 23, average loss = 1.4856771230697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:13,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.5201823711395264\u001b[0m\n",
      "\u001b[35m[1,mpirank:9,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.5061849355697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:14,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.5305989980697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:11,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.5169271230697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:12,algo-2]<stdout>:--DD-- global_step = 23, average loss = 1.4807943105697632\u001b[0m\n",
      "\u001b[35m[1,mpirank:0,algo-1]<stdout>:--DD-- SMP training finished successfully\u001b[0m\n",
      "\u001b[35m2023-05-26 07:55:07,675 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:55:07,675 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-26 07:55:07,675 sagemaker-training-toolkit INFO     Begin writing status file from leader node to worker nodes (if any)\u001b[0m\n",
      "\u001b[35m2023-05-26 07:55:07,675 sagemaker-training-toolkit INFO     Start writing mpirun finished status to algo-1\u001b[0m\n",
      "\u001b[35m2023-05-26 07:55:07,823 sagemaker-training-toolkit INFO     output from subprocess run CompletedProcess(args=['ssh', 'algo-1', 'touch', '/tmp/done.algo-2'], returncode=0, stdout='', stderr='')\u001b[0m\n",
      "\u001b[35m2023-05-26 07:55:07,824 sagemaker-training-toolkit INFO     Finished writing status file\u001b[0m\n",
      "\u001b[34m2023-05-26 07:55:07,676 sagemaker-training-toolkit INFO     Invoked on_terminate from psutil.wait_for_procs\u001b[0m\n",
      "\u001b[34m2023-05-26 07:55:07,677 sagemaker-training-toolkit INFO     process psutil.Process(pid=107, name='orted', status='terminated', started='07:48:57') terminated with exit code None\u001b[0m\n",
      "\u001b[34m2023-05-26 07:55:07,677 sagemaker-training-toolkit INFO     Reporting status for ORTEd process. gone: [psutil.Process(pid=107, name='orted', status='terminated', started='07:48:57')] alive: []\u001b[0m\n",
      "\u001b[34m2023-05-26 07:55:07,677 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# change to fp16=false, TBD: 1/model creation without wrapper; 2/no barrier in cache data\n",
    "dat_chnl = {'train123':'s3://llm-artifacts-us-east-1/datasets/coig/'}\n",
    "est.fit(dat_chnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcce0b-1a02-4821-9838-42d196d34120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42db27f-47a9-4db9-80ae-898ce8bb39ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
